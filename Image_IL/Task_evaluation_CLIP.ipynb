{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_to_add = '/home/jin/SRC-gym/gym-env/Hierachical_Learning_v2'\n",
    "sys.path.append(path_to_add)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from cv_bridge import CvBridge\n",
    "import rospy\n",
    "from sensor_msgs.msg import Image as RosImage\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from Insert_env import SRC_insert  # To be modified\n",
    "import time\n",
    "import clip\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define the BehaviorCloningModel class as before\n",
    "class BehaviorCloningModel(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super(BehaviorCloningModel, self).__init__()\n",
    "        self.clip = clip_model\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.BatchNorm1d(512 + 7),\n",
    "            nn.Linear(512 + 7, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 7),\n",
    "            nn.Tanh()  # Apply Tanh to restrict output range to (-1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, proprioceptive_data):\n",
    "        with torch.no_grad():\n",
    "            image_features = self.clip.encode_image(x)\n",
    "            image_features = image_features.view(image_features.size(0), -1)\n",
    "        combined_input = torch.cat((image_features, proprioceptive_data), dim=1)\n",
    "        return self.regressor(combined_input)\n",
    "\n",
    "# Load the CLIP model and preprocess\n",
    "def load_clip_model():\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    model.eval()\n",
    "    return model, preprocess\n",
    "\n",
    "clip_model, preprocess = load_clip_model()\n",
    "\n",
    "# Function to load the trained model\n",
    "def load_trained_model(model_path, clip_model):\n",
    "    model = BehaviorCloningModel(clip_model).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Function to predict action using the loaded model\n",
    "def predict_action(model, image_np, proprio_data):\n",
    "    # Ensure image is in the correct format (H, W, C) and normalize it\n",
    "    image_tensor = preprocess(Image.fromarray(image_np)).unsqueeze(0).to(device)  # Convert image to tensor and add batch dimension\n",
    "    proprioceptive_tensor = torch.tensor(proprio_data, dtype=torch.float32).unsqueeze(0).to(device)  # Ensure proprioceptive data is processed correctly\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted_action = model(image_tensor, proprioceptive_tensor)\n",
    "    return predicted_action.cpu().numpy()\n",
    "\n",
    "# Initialize ROS node\n",
    "current_images = {}\n",
    "image_received = {}\n",
    "bridge = CvBridge()\n",
    "view_name = 'back'      # front, back\n",
    "task_name = 'Insert'  # Approach, Place, Insert, Regrasp, Pullout\n",
    "algor_name = 'CLIP'     # CLIP only for this script, do not change this value!!!\n",
    "\n",
    "def image_callback(msg, camera_id):\n",
    "    \"\"\"Callback to process and save images from different cameras.\"\"\"\n",
    "    global current_images, image_received\n",
    "    try:\n",
    "        current_images[camera_id] = bridge.imgmsg_to_cv2(msg, \"bgr8\")\n",
    "        image_received[camera_id] = True\n",
    "    except Exception as e:\n",
    "        rospy.logerr(f\"Failed to convert image from {camera_id}: {e}\")\n",
    "\n",
    "if view_name == 'front':\n",
    "    camera_topics = {\n",
    "        'front': '/ambf/env/cameras/cameraL/ImageData'\n",
    "    }\n",
    "\n",
    "elif view_name == 'back':\n",
    "    camera_topics = {\n",
    "        'back': '/ambf/env/cameras/normal_camera/ImageData'\n",
    "    }\n",
    "\n",
    "for cam_id, topic in camera_topics.items():\n",
    "    rospy.Subscriber(topic, RosImage, image_callback, callback_args=(cam_id))\n",
    "    image_received[cam_id] = False\n",
    "\n",
    "def wait_for_images():\n",
    "    \"\"\"Wait for all cameras to have received an image.\"\"\"\n",
    "    rate = rospy.Rate(100)\n",
    "    while not all(image_received.values()) and not rospy.is_shutdown():\n",
    "        rate.sleep()\n",
    "    for key in image_received:\n",
    "        image_received[key] = False\n",
    "\n",
    "# Example usage\n",
    "model_path = f'/home/jin/SRC-gym/gym-env/Hierachical_Learning_v2/SRC_img_data/{task_name}/{algor_name}/{view_name}/model_final.pth'\n",
    "model = load_trained_model(model_path, clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 60\n",
    "set_random_seed(seed)\n",
    "\n",
    "max_episode_steps=500\n",
    "trans_step = 0.05e-2  # Trans unit in m\n",
    "angle_step = np.deg2rad(2)\n",
    "jaw_step = 0.05\n",
    "threshold = [0.3,np.deg2rad(10)]   # Trans unit in cm\n",
    "\n",
    "step_size = np.array([trans_step,trans_step,trans_step,angle_step,angle_step,angle_step,jaw_step],dtype=np.float32) \n",
    "####################\n",
    "\n",
    "threshold_expert = [0.1,np.deg2rad(5)] \n",
    "gym.envs.register(id=\"TD3_HER_sparse\", entry_point=SRC_insert, max_episode_steps=max_episode_steps)\n",
    "env = gym.make(\"TD3_HER_sparse\", render_mode=None,reward_type = \"dense\",seed = seed, threshold = threshold_expert,max_episode_step=max_episode_steps,step_size=step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 3\n",
    "for episode in range(num_episodes):\n",
    "    obs,_ = env.reset()\n",
    "    time.sleep(0.5)\n",
    "    for timestep in range(max_episode_steps):\n",
    "        wait_for_images()  # Wait until an image is received\n",
    "        proprio_data = obs[\"observation\"][0:7]\n",
    "        action = predict_action(model, current_images[view_name], proprio_data).squeeze()\n",
    "        next_obs, reward, done, _, info = env.step(action)\n",
    "        time.sleep(0.01)\n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            print(timestep)\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
