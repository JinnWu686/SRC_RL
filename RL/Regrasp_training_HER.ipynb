{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from Regrasp_env import SRC_regrasp\n",
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import HerReplayBuffer, DDPG, PPO, SAC\n",
    "from RL_algo.DDPG_BC import DDPG_BC\n",
    "from RL_algo.td3_BC import TD3_BC\n",
    "from RL_algo.DemoHerReplayBuffer import DemoHerReplayBuffer\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "import time\n",
    "import pickle\n",
    "# Create environment\n",
    "\n",
    "seed = 10\n",
    "set_random_seed(seed)\n",
    "\n",
    "with open('./Regrasp_noise_env_info', 'rb') as file:\n",
    "    env_info = pickle.load(file)\n",
    "    \n",
    "step_size= np.array(env_info[\"step_size\"], dtype=np.float32)\n",
    "threshold = np.array(env_info[\"threshold\"], dtype=np.float32)\n",
    "episode_steps = int(env_info[\"max_timestep\"])\n",
    "\n",
    "gym.envs.register(id=\"TD3_HER_BC\", entry_point=SRC_regrasp, max_episode_steps=episode_steps)\n",
    "env = gym.make(\"TD3_HER_BC\", render_mode=\"human\",reward_type = \"dense\",max_episode_step=episode_steps,seed = seed, step_size=step_size,threshold=threshold)\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the environment\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for _ in range(20000):\n",
    "    # Random action\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./Regrasp_noise_Expert_100.pkl', 'rb') as file:\n",
    "    episode_transitions = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "goal_selection_strategy = \"future\"\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=5e-2 * np.ones(n_actions))\n",
    "\n",
    "\n",
    "# Viable param\n",
    "model = TD3_BC(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    learning_rate=3e-4,\n",
    "    learning_starts=600,\n",
    "    tau = 0.005,\n",
    "    gamma = 0.995,\n",
    "    batch_size=512,\n",
    "    action_noise=action_noise,\n",
    "    replay_buffer_class=DemoHerReplayBuffer,\n",
    "    train_freq = (3, \"episode\"),\n",
    "    policy_kwargs = dict(net_arch=dict(pi=[256, 256, 256], qf=[256, 256, 256])),\n",
    "    replay_buffer_kwargs=dict(\n",
    "        demo_transitions=episode_transitions, \n",
    "        demo_sample_ratio=0.3,\n",
    "        n_sampled_goal=4,\n",
    "        goal_selection_strategy=goal_selection_strategy,\n",
    "    ),\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./Regrasp/TD3_BC_noise_dense\",\n",
    "    episode_transitions=episode_transitions,\n",
    "    BC_coeff=0.8,\n",
    "    demo_ratio=0.2,\n",
    ")\n",
    "\n",
    "# model_path = \"./Regrasp/TD3_BC_noise_dense/rl_model_final\"\n",
    "# model = TD3_BC.load(model_path,env=env)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=5000, save_path='./Regrasp/TD3_BC_noise_dense', name_prefix='rl_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=int(800000), progress_bar=True,callback=checkpoint_callback,reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./Regrasp/TD3_BC_noise_dense/rl_model_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the action with model\n",
    "obs,info = env.reset()\n",
    "print(obs)\n",
    "for i in range(90000):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    achieve_obs = obs[\"achieved_goal\"]\n",
    "    desired_obs = obs[\"desired_goal\"]\n",
    "    time.sleep(0.05)\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
